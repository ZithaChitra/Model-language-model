diff --git a/nd_lang/Ndebele_nd_languuage_model.egg-info/SOURCES.txt b/nd_lang/Ndebele_nd_languuage_model.egg-info/SOURCES.txt
index 6b25193..c2ea316 100644
--- a/nd_lang/Ndebele_nd_languuage_model.egg-info/SOURCES.txt
+++ b/nd_lang/Ndebele_nd_languuage_model.egg-info/SOURCES.txt
@@ -5,7 +5,6 @@ Ndebele_nd_languuage_model.egg-info/dependency_links.txt
 Ndebele_nd_languuage_model.egg-info/requires.txt
 Ndebele_nd_languuage_model.egg-info/top_level.txt
 lab1/__init__.py
-lab1/loader.py
 lab1/language_model/__init__.py
 lab1/language_model/ndebele_text_generator.py
 lab1/language_model/util.py
diff --git a/nd_lang/initialized_model.keras b/nd_lang/initialized_model.keras
index 53406ef..b962ce9 100644
Binary files a/nd_lang/initialized_model.keras and b/nd_lang/initialized_model.keras differ
diff --git a/nd_lang/lab1/language_model/models/__pycache__/base2.cpython-38.pyc b/nd_lang/lab1/language_model/models/__pycache__/base2.cpython-38.pyc
index a964cc1..3810032 100644
Binary files a/nd_lang/lab1/language_model/models/__pycache__/base2.cpython-38.pyc and b/nd_lang/lab1/language_model/models/__pycache__/base2.cpython-38.pyc differ
diff --git a/nd_lang/lab1/language_model/models/base2.py b/nd_lang/lab1/language_model/models/base2.py
index 0178eab..dd94d03 100644
--- a/nd_lang/lab1/language_model/models/base2.py
+++ b/nd_lang/lab1/language_model/models/base2.py
@@ -17,7 +17,20 @@ DIRNAME = Path(__file__).parents[1].resolve() / "weights"
 class Model():
     """ 
 	Base class, to be subclassed by predictors for specific types of data.
-	This is a wrapper that 
+	This is a wrapper that makes  it convinient to use different neural net
+	configurations during experiments. Configurations could be a different
+	neural net archicture, dataset or maybe just hyperparameters.
+	
+	Parameters:
+	----------
+	dataset_cls: type
+		Name of class that interfaces with your dataset.
+	network_fn: Callable[..., KerasModel]
+		Name of function that returns the KerasModel to be used for training.
+	dataset_args: Dict
+		A dictionary of arguments for modifying the dataset
+	network_args:
+		A dictionary of arguments for creating model
 	"""
     def __init__(
         self,
diff --git a/nd_lang/lab1/language_model/networks/__pycache__/mlp.cpython-38.pyc b/nd_lang/lab1/language_model/networks/__pycache__/mlp.cpython-38.pyc
index 04951df..d3e4159 100644
Binary files a/nd_lang/lab1/language_model/networks/__pycache__/mlp.cpython-38.pyc and b/nd_lang/lab1/language_model/networks/__pycache__/mlp.cpython-38.pyc differ
diff --git a/nd_lang/lab1/loader.py b/nd_lang/lab1/loader.py
deleted file mode 100644
index 5e2c634..0000000
--- a/nd_lang/lab1/loader.py
+++ /dev/null
@@ -1,5 +0,0 @@
-import mlflow 
-
-loaded_model = mlflow.pyfunc.load_model("my_model")
-
-print("ok!")
diff --git a/nd_lang/lab1/training/__pycache__/util.cpython-38.pyc b/nd_lang/lab1/training/__pycache__/util.cpython-38.pyc
index 5c9b28c..a15f502 100644
Binary files a/nd_lang/lab1/training/__pycache__/util.cpython-38.pyc and b/nd_lang/lab1/training/__pycache__/util.cpython-38.pyc differ
diff --git a/nd_lang/lab1/training/__pycache__/util_yaml.cpython-38.pyc b/nd_lang/lab1/training/__pycache__/util_yaml.cpython-38.pyc
index 354c5dd..7b7d540 100644
Binary files a/nd_lang/lab1/training/__pycache__/util_yaml.cpython-38.pyc and b/nd_lang/lab1/training/__pycache__/util_yaml.cpython-38.pyc differ
diff --git a/nd_lang/lab1/training/run_experiment2.py b/nd_lang/lab1/training/run_experiment2.py
index 57cbd1a..37feb63 100644
--- a/nd_lang/lab1/training/run_experiment2.py
+++ b/nd_lang/lab1/training/run_experiment2.py
@@ -6,7 +6,7 @@ import click
 
 # from lab1.training.util import train_model
 from lab1.training.util import save_net_artifact, save_data_raw_artifact, save_data_processed_artifact
-from lab1.training.util_yaml import yaml_loader
+from lab1.training.util_yaml import yaml_loader, yaml_dump
 import wandb
 from wandb.keras import WandbCallback
 # import numpy as np
@@ -16,10 +16,11 @@ DEFAULT_TRAIN_ARGS = {"batch_size":64, "epochs":16}
 
 
 @click.command()
-@click.argument("exp_config", type=click.Path(exists=True),)
-@click.argument("sweep_config", type=click.Path(exists=True),)
-@click.option("--train-args", default=DEFAULT_TRAIN_ARGS)
-def run_experiment(exp_config, sweep_config, train_args):
+@click.argument("exp-config-yaml", type=click.Path(exists=True), default="yamls/experiments/default.yaml")
+@click.option("--epochs")
+@click.option("--activation-fn")
+@click.option("--optimizer")
+def run_experiment(exp_config_yaml, epochs, activation_fn, optimizer):
 	"""
 	Run a single experiment.
 	Parameters:
@@ -35,8 +36,14 @@ def run_experiment(exp_config, sweep_config, train_args):
 	"""
 
 
-	exp_config = yaml_loader(exp_config)
-    
+	exp_config = yaml_loader(exp_config_yaml)
+	exp_config["network"]["network_args"]["hyperparams"]["epochs"] = epochs
+	exp_config["network"]["network_args"]["hyperparams"]["activation_fn"] = activation_fn
+	# exp_config["network"]["network_args"]["hyperparams"]["optimizer"] = optimizer
+	yaml_dump(exp_config_yaml, exp_config)
+
+
+
 	model = exp_config.get("model")
 
 	network = exp_config.get("network")
@@ -66,6 +73,7 @@ def run_experiment(exp_config, sweep_config, train_args):
 	model = model_class_(dataset_cls=dataset_class_, network_fn=network_fn, dataset_args=dataset_args, network_args=net_config)
 
 
+
 	# mlflow.set_tracking_uri("sqlite:///mlruns.db")
 	# input_schema = Schema([TensorSpec(type=np.dtype(np.float32), shape=(-1, 13), name="house_attribs")])
 	# output_schema = Schema([TensorSpec(type=np.dtype(np.float32), shape=(-1, 1), name="predicted house price")])
@@ -81,6 +89,10 @@ def run_experiment(exp_config, sweep_config, train_args):
 		config = wandb.config
         
 		model.fit(dataset=config.dataset, callbacks=[WandbCallback()])
+		data = data_cl_name()
+		mse = model.network.evaluate(data.X_tr, data.y_tr)
+		wandb.log({"mse": mse})
+		
 
 
 	# model_ = train_model(
diff --git a/nd_lang/lab1/training/util_yaml.py b/nd_lang/lab1/training/util_yaml.py
index 4776c10..5373bb3 100644
--- a/nd_lang/lab1/training/util_yaml.py
+++ b/nd_lang/lab1/training/util_yaml.py
@@ -31,6 +31,9 @@ def yaml_dump(filepath, data):
 
 
 
+
+
+
 
 
 
diff --git a/nd_lang/wandb/debug-internal.log b/nd_lang/wandb/debug-internal.log
index 6ee6983..bd257d7 120000
--- a/nd_lang/wandb/debug-internal.log
+++ b/nd_lang/wandb/debug-internal.log
@@ -1 +1 @@
-run-20210411_095544-8mm3x4ht/logs/debug-internal.log
\ No newline at end of file
+run-20210417_122633-0kucxvmn/logs/debug-internal.log
\ No newline at end of file
diff --git a/nd_lang/wandb/debug.log b/nd_lang/wandb/debug.log
index 66947e3..82a78e6 120000
--- a/nd_lang/wandb/debug.log
+++ b/nd_lang/wandb/debug.log
@@ -1 +1 @@
-run-20210411_095544-8mm3x4ht/logs/debug.log
\ No newline at end of file
+run-20210417_122633-0kucxvmn/logs/debug.log
\ No newline at end of file
diff --git a/nd_lang/wandb/latest-run b/nd_lang/wandb/latest-run
index d3ebc6d..d5abedb 120000
--- a/nd_lang/wandb/latest-run
+++ b/nd_lang/wandb/latest-run
@@ -1 +1 @@
-run-20210411_095544-8mm3x4ht
\ No newline at end of file
+run-20210417_122633-0kucxvmn
\ No newline at end of file
diff --git a/nd_lang/yamls/experiments/default.yaml b/nd_lang/yamls/experiments/default.yaml
index d50d543..9bf5246 100644
--- a/nd_lang/yamls/experiments/default.yaml
+++ b/nd_lang/yamls/experiments/default.yaml
@@ -8,7 +8,7 @@ network:
     hyperparams:
       activation_fn: relu
       dropout_amount: 0.2
-      epochs: 15
+      epochs: '20'
       layer_size: 64
       learning_rate: 0.03
       num_layers: 4
